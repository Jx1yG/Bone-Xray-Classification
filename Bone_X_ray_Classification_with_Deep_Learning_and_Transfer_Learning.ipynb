{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Define image size and dataset path\n",
        "image_size = (224, 224)\n",
        "dataset_path = 'MURA-v1.1'\n",
        "\n",
        "# Define body parts in the dataset\n",
        "body_parts = ['XR_ELBOW', 'XR_FINGER', 'XR_FOREARM','XR_HAND']\n",
        "\n",
        "\n",
        "# Function to extract labels\n",
        "def get_labels(image_path):\n",
        "    parts = image_path.split('/')\n",
        "    body_part = parts[-4]\n",
        "    patient_id = parts[-3]\n",
        "    abnormality = 1 if 'positive' in parts[-2] else 0\n",
        "    return patient_id, body_part, abnormality\n",
        "\n",
        "# Function to load images and labels from a given dataset path\n",
        "def load_dataset(dataset_path):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    abnormality_labels = []\n",
        "    all_images = []\n",
        "    body_part_labels = [] # Initialize body_part_labels here\n",
        "    patient_ids = []\n",
        "\n",
        "    # Get all image paths from the dataset directory\n",
        "    for root, _, files in os.walk(dataset_path):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):  # Check for common image extensions\n",
        "                image_paths.append(os.path.join(root, file))\n",
        "\n",
        "    for image_path in image_paths:\n",
        "        for body_part in body_parts:\n",
        "            if body_part in image_path:\n",
        "\n",
        "                patient_id, body_part, abnormality = get_labels(image_path)\n",
        "                body_part_labels.append(body_part)\n",
        "                abnormality_labels.append(abnormality)\n",
        "\n",
        "                img = Image.open(image_path).convert('RGB').resize((224, 224)) # Assuming image_size is (224, 224)\n",
        "                img_array = np.array(img)\n",
        "                all_images.append(img_array)\n",
        "    return all_images, abnormality_labels, body_part_labels\n",
        "\n",
        "all_images, abnormality_labels, body_part_labels = load_dataset(dataset_path)\n",
        "\n",
        "index_to_display = 0\n",
        "plt.imshow(all_images[index_to_display])\n",
        "plt.title(f\"Abnormality Label: {abnormality_labels[index_to_display]}, Body Part: {body_part_labels[index_to_display]}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "all_images = np.array(all_images)\n",
        "abnormality_labels = np.array(abnormality_labels)\n",
        "body_part_labels_encoded = np.array(body_part_labels)\n",
        "\n",
        "\n",
        "# Define training and validation datasets\n",
        "train_dataset_path = '/MURA-v1.1/train'\n",
        "valid_dataset_path = '/MURA-v1.1/valid'\n",
        "\n",
        "# Load training and validation datasets\n",
        "x_train, y_train_abnormality, y_train_body_part = load_dataset(train_dataset_path)\n",
        "x_val, y_val_abnormality, y_val_body_part = load_dataset(valid_dataset_path)\n",
        "\n",
        "\n",
        "print(f\"Total number of images loaded: {len(all_images)}\")\n",
        "\n",
        "\n",
        "\n",
        "# One-hot encode body part labels\n",
        "body_part_labels = [str(label) for label in body_part_labels]\n",
        "encoder = LabelBinarizer()\n",
        "body_part_labels_encoded = encoder.fit_transform(body_part_labels)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "x_train, x_val, y_train_abnormality, y_val_abnormality, y_train_body_part, y_val_body_part = train_test_split(\n",
        "    all_images, abnormality_labels, body_part_labels_encoded, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Normalize pixel values\n",
        "x_train = x_train / 255.0\n",
        "x_val = x_val / 255.0\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# Transfer learning with ResNet50\n",
        "base_model = tf.keras.applications.ResNet50(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "base_model.trainable = False  # Freeze pre-trained layers\n",
        "\n",
        "# Build the CNN model\n",
        "image_input = layers.Input(shape=(224, 224, 3))\n",
        "\n",
        "x = base_model(image_input, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "abnormality_output = layers.Dense(1, activation='sigmoid', name='abnormality')(x)\n",
        "body_part_output = layers.Dense(y_train_body_part.shape[1], activation='softmax', name='body_part')(x)\n",
        "\n",
        "model = models.Model(inputs=image_input, outputs=[abnormality_output, body_part_output])\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
        "              loss={'abnormality': tf.keras.losses.BinaryCrossentropy(), 'body_part': tf.keras.losses.CategoricalCrossentropy()},\n",
        "              metrics={'abnormality': [tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
        "                       'body_part': tf.keras.metrics.CategoricalAccuracy()})\n",
        "\n",
        "# Concatenate the labels into a single array\n",
        "y_train_abnormality = np.array(y_train_abnormality)\n",
        "y_val_abnormality = np.array(y_val_abnormality)\n",
        "y_train_body_part = np.array(y_train_body_part)\n",
        "y_val_body_part = np.array(y_val_body_part)\n",
        "\n",
        "y_train = np.concatenate((y_train_abnormality.reshape(-1, 1), y_train_body_part), axis=1)\n",
        "y_val = np.concatenate((y_val_abnormality.reshape(-1, 1), y_val_body_part), axis=1)\n",
        "\n",
        "#Concatenate the labels into a single array\n",
        "\n",
        "y_train = np.concatenate((y_train_abnormality.reshape(-1, 1), y_train_body_part), axis=1)\n",
        "y_val = np.concatenate((y_val_abnormality.reshape(-1, 1), y_val_body_part), axis=1)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    {'abnormality': y_train_abnormality, 'body_part': y_train_body_part},\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(x_val, {'abnormality': y_val_abnormality, 'body_part': y_val_body_part})\n",
        ")\n",
        "\n",
        "\n",
        "# Visualize training and validation accuracy\n",
        "plt.plot(history.history['abnormality_binary_accuracy'], label='Abnormality Accuracy')\n",
        "plt.plot(history.history['val_abnormality_binary_accuracy'], label='Validation Abnormality Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['body_part_categorical_accuracy'], label='Body Part Accuracy')\n",
        "plt.plot(history.history['val_body_part_categorical_accuracy'], label='Validation Body Part Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Final Abnormality Accuracy:\", history.history['abnormality_binary_accuracy'][-1])\n",
        "print(\"Final Validation Abnormality Accuracy:\", history.history['val_abnormality_binary_accuracy'][-1])\n",
        "\n",
        "\n",
        "#test the model with a new xray image\n",
        "\n",
        "def predict_new_image(image_path):\n",
        "  img = Image.open(image_path).convert('RGB').resize((224, 224))\n",
        "  img_array = np.array(img) / 255.0\n",
        "  img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "  predictions = model.predict(img_array)\n",
        "  predicted_abnormality = (predictions[0] > 0.5).astype(int)[0][0]\n",
        "  predicted_body_part_probs = predictions[1][0]\n",
        "  predicted_body_part_index = np.argmax(predicted_body_part_probs)\n",
        "  predicted_body_part = encoder.classes_[predicted_body_part_index]\n",
        "\n",
        "  return predicted_abnormality, predicted_body_part\n",
        "\n",
        "# Example usage:\n",
        "new_image_path = 'finger.png'\n",
        "predicted_abnormality, predicted_body_part = predict_new_image(new_image_path)\n",
        "\n",
        "print(f\"Predicted Abnormality: {predicted_abnormality}\")\n",
        "print(f\"Predicted Body Part: {predicted_body_part}\")\n"
      ],
      "metadata": {
        "id": "X5WXAvz5RW7O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}